{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Indoor Scene Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wjv0TNRihOdI"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananda-2/6-Age-Calculator/blob/main/Indoor_Scene_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indoor Scene Recognition"
      ],
      "metadata": {
        "id": "3f0fKuH7m8tC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has a collection of about 15000+ labeled images belonging to 67 categories. I am selecting only below 10 categories âš°\n",
        "\n",
        "airport_inside, auditorium, bakery, bathroom, bookstore, casino, church_inside, grocerystore, operating_room, warehouse\n",
        "\n",
        "Objective is to create a model that will able to classify images into these 10 categories.\n"
      ],
      "metadata": {
        "id": "BbX7muvvmPwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing required libraries\n"
      ],
      "metadata": {
        "id": "BEYDiZRjEf5t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "85_Em8MBrc24"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import os\n",
        "import pathlib\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import random as rn\n",
        "rn.seed(42)\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Lambda, Dense, Flatten, GlobalMaxPool3D, BatchNormalization, Dropout, Activation\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "from keras.regularizers import l2\n",
        "from keras.models import load_model\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking available Nvidia GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDYOVev86VHR",
        "outputId": "cf02714b-afc8-458a-c0d4-bdc9dac03f14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 18 05:46:27 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing data"
      ],
      "metadata": {
        "id": "xfJgRUp4EmG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Unzip files\n",
        "# from zipfile import ZipFile\n",
        "\n",
        "# with ZipFile('/content/drive/MyDrive/colab_data/indoorCVPR/data_10_cats.zip', 'r') as zipobj:\n",
        "#     zipobj.extractall('/content/drive/MyDrive/colab_data/indoorCVPR')\n",
        "#     print('Files are unzipped')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t3HvxbYn7Rrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have considered below 10 categories for classification:\n",
        "airport_inside, auditorium, bakery, bathroom, bookstore, casino, church_inside, grocerystore, operating_room, warehouse"
      ],
      "metadata": {
        "id": "MIHBTnt-CsHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spliting data for training and validation"
      ],
      "metadata": {
        "id": "Wodxi2KCErpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training dataset path\n",
        "data_path = '/content/drive/My Drive/archive/indoorCVPR_09/Images'\n",
        "train_data_dir = pathlib.Path(data_path)\n",
        "\n",
        "# Counting the number of images\n",
        "image_count = len(list(train_data_dir.glob('*/*.jpg')))\n",
        "print(f\"Total images: {image_count}\")"
      ],
      "metadata": {
        "id": "c1ZdJGVTBBeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting image height, width and batch size\n",
        "img_height= 150\n",
        "img_width= 150\n",
        "batch_size= 64"
      ],
      "metadata": {
        "id": "7H8_gUrtDSvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training dataset\n",
        "train_ds= tf.keras.preprocessing.image_dataset_from_directory(train_data_dir, seed= 42, validation_split= 0.2,\n",
        "                                                              subset= 'training',\n",
        "                                                              batch_size= batch_size,\n",
        "                                                              image_size= (img_height, img_width)\n",
        "                                                              )"
      ],
      "metadata": {
        "id": "J5O7C8GjCh7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation dataset\n",
        "val_ds= tf.keras.preprocessing.image_dataset_from_directory(train_data_dir, seed= 42, validation_split= 0.2,\n",
        "                                                              subset= 'validation',\n",
        "                                                              batch_size= batch_size,\n",
        "                                                              image_size= (img_height, img_width)\n",
        "                                                              )"
      ],
      "metadata": {
        "id": "bvn8qs6-DGHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vizualizing training data"
      ],
      "metadata": {
        "id": "27_q-jRNKAem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking all class names\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "dFyKdV5zEWJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking training data\n",
        "plt.figure(figsize=(9, 9))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(16):\n",
        "    ax = plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype('uint8'))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "QY1d-JOgFJGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation"
      ],
      "metadata": {
        "id": "HeDUYXjsLAmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, creating data augmentation layer. I have selected only relevant data augmentation techniques for current scenario."
      ],
      "metadata": {
        "id": "c0_7QM7NnndP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Creating data augmentation layer\n",
        "augmentation_layer= Sequential(\n",
        "  [tf.keras.layers.InputLayer(input_shape= (img_height, img_width, 3)),\n",
        "   tf.keras.layers.RandomFlip('horizontal', seed= 42),\n",
        "   tf.keras.layers.RandomRotation(0.2, fill_mode= 'wrap', seed= 42),\n",
        "   tf.keras.layers.RandomZoom(0.2, seed= 42),\n",
        "   tf.keras.layers.RandomTranslation(.2, .2, fill_mode='wrap', interpolation='bilinear', seed= 42),\n",
        "   tf.keras.layers.RandomContrast(0.2, seed= 42)]\n",
        "   )"
      ],
      "metadata": {
        "id": "7A8vQCJrHlx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting different augmented version of a random image from training dataset\n",
        "plt.figure(figsize=(9, 9))\n",
        "a= np.random.randint(42)\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range(12):\n",
        "      aug_img= augmentation_layer(images)\n",
        "      ax = plt.subplot(3, 4, i + 1)\n",
        "      plt.imshow(aug_img[a].numpy().astype(np.int32))\n",
        "      plt.axis('off')"
      ],
      "metadata": {
        "id": "aagLKYgOL2I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model building"
      ],
      "metadata": {
        "id": "Yt0Pwa7CNmff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### InceptionV3 Finetuning"
      ],
      "metadata": {
        "id": "wjv0TNRihOdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First trying transfer learning using InceptionV3 architecture with imagenet pretrained weights. I have removed the default output softmax layer of InceptionV3. I have kept First 249 layers weights as it is. Trained layer 249 to last layer on training dataset. Added a Flatten layer, Dropout layers, 2 hidden dense layers and output dense layer with 10 neurons and softmax as activation.\n",
        "\n",
        "I will use label encoding instead of one hot encoding to optimize memory utilization. So my loss function will be: sparse_categorical_entropy and my metric will be sparse_categorical_accuracy."
      ],
      "metadata": {
        "id": "6hVD_MEZpHLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating function to fine tune InceptionV3\n",
        "def inceptionv3(inp_shape, dropout_rate, train_layers_after):\n",
        "    inceptionv3= InceptionV3(weights= 'imagenet', include_top= False)\n",
        "    for layer in inceptionv3.layers[:train_layers_after]:\n",
        "      layer.trainable= False\n",
        "    input_layer= Input(shape= inp_shape)\n",
        "    data_aug_layer= augmentation_layer(input_layer)\n",
        "    norm_layer= tf.keras.layers.Rescaling(1./255)(data_aug_layer)\n",
        "    cnn_layers= inceptionv3(norm_layer)\n",
        "    flatten_layer= Flatten()(cnn_layers)\n",
        "    dropout_layer1= Dropout(dropout_rate)(flatten_layer)\n",
        "    dense_layer= Dense(1024, activation= 'relu', kernel_initializer= 'he_normal', kernel_regularizer='l2')(dropout_layer1)\n",
        "    dropout_layer2= Dropout(dropout_rate)(dense_layer)\n",
        "    dense_layer_1= Dense(512, activation= 'relu', kernel_initializer= 'he_normal', kernel_regularizer='l2')(dropout_layer2)\n",
        "    dropout_layer3= Dropout(dropout_rate)(dense_layer_1)\n",
        "    output_layer= Dense(10, activation= 'softmax')(dropout_layer3)\n",
        "    model= Model(input_layer, output_layer)\n",
        "    model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics= ['sparse_categorical_accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "EOdSKZ6cNohe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating model\n",
        "inp_shape= (150, 150, 3)\n",
        "dropout_rate= .25\n",
        "train_layers_after= 249\n",
        "incv3_model= inceptionv3(inp_shape, dropout_rate, train_layers_after)\n",
        "incv3_model.summary()"
      ],
      "metadata": {
        "id": "fnTwYe_hRIu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data into cache to overcome data bottleneck during training.\n",
        "AUTOTUNE= tf.data.AUTOTUNE\n",
        "AUTOTUNE= tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Shuffling data before starting of each epoch\n",
        "train_ds= train_ds.cache().shuffle(1000).prefetch(buffer_size= AUTOTUNE)\n",
        "val_ds= val_ds.cache().prefetch(buffer_size= AUTOTUNE)"
      ],
      "metadata": {
        "id": "6uPSrFGhWvri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting callbacks\n",
        "\n",
        "base_path= '/content/drive/MyDrive/colab_data/indoorCVPR/models/'\n",
        "\n",
        "filepath= base_path + 'model-{epoch:05d}-{loss:.5f}-{sparse_categorical_accuracy:.5f}-{val_loss:.5f}-{val_sparse_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint= ModelCheckpoint(filepath, monitor= 'val_sparse_categorical_accuracy', verbose= 1,\n",
        "                            save_best_only= True, save_weights_only= False, mode= 'auto')\n",
        "\n",
        "LR= ReduceLROnPlateau(monitor= 'val_loss', factor= 0.1, patience= 30, verbose= 1)\n",
        "callbacks_list= [checkpoint, LR]"
      ],
      "metadata": {
        "id": "198OmSMsSwLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "epochs= 200\n",
        "steps_per_epoch= math.ceil(2652/batch_size)\n",
        "validation_steps= math.ceil(663/batch_size)\n",
        "\n",
        "history= incv3_model.fit(train_ds, validation_data= val_ds, steps_per_epoch= steps_per_epoch, epochs= epochs, callbacks= callbacks_list, validation_steps= validation_steps)"
      ],
      "metadata": {
        "id": "HO8-WWTfRZeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc= history.history['sparse_categorical_accuracy']\n",
        "val_acc= history.history['val_sparse_categorical_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "03Z9jrHgSPmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Best Epoch: 109 (model-00109-0.08159-0.99170-0.67942-0.85068.h5)\n",
        "- **Training: loss: .08 - categorical_accuracy: 0.99**\n",
        "- **Validation: val_loss: .68 - val_sparse_categorical_accuracy: 0.85**\n"
      ],
      "metadata": {
        "id": "xghIIFQyk6_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xception Finetunning"
      ],
      "metadata": {
        "id": "-s0AEYxSjHJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used almost same architecture as previous one, only used Xception as our cnn architecture instead of InceptionV3. Removed output softmax layer of Xception architecture and finetunned Xception from layer 114 to end layer. Then I have added custom layers as previous."
      ],
      "metadata": {
        "id": "XOwP6xe0qVRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating function to fine tune Xception\n",
        "def xception_cnn(inp_shape, dropout_rate, train_layers_after):\n",
        "    xcp= Xception(weights= 'imagenet', include_top= False)\n",
        "    for layer in xcp.layers[:train_layers_after]:\n",
        "      layer.trainable= False\n",
        "    input_layer= Input(shape= inp_shape)\n",
        "    data_aug_layer= augmentation_layer(input_layer)\n",
        "    norm_layer= tf.keras.layers.Rescaling(1./255)(data_aug_layer)\n",
        "    cnn_layers= xcp(norm_layer)\n",
        "    flatten_layer= Flatten()(cnn_layers)\n",
        "    dropout_layer1= Dropout(dropout_rate)(flatten_layer)\n",
        "    dense_layer= Dense(1024, activation= 'relu', kernel_initializer= 'he_normal', kernel_regularizer='l2')(dropout_layer1)\n",
        "    dropout_layer2= Dropout(dropout_rate)(dense_layer)\n",
        "    dense_layer_1= Dense(512, activation= 'relu', kernel_initializer= 'he_normal', kernel_regularizer='l2')(dropout_layer2)\n",
        "    dropout_layer3= Dropout(dropout_rate)(dense_layer_1)\n",
        "    output_layer= Dense(10, activation= 'softmax')(dropout_layer3)\n",
        "    model= Model(input_layer, output_layer)\n",
        "    model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics= ['sparse_categorical_accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "lOZnUf-WjSvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating model\n",
        "inp_shape= (150, 150, 3)\n",
        "dropout_rate= .25\n",
        "train_layers_after= 114\n",
        "xcp_model= xception_cnn(inp_shape, dropout_rate, train_layers_after)\n",
        "xcp_model.summary()"
      ],
      "metadata": {
        "id": "hVXClRr6j-pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "epochs= 10\n",
        "steps_per_epoch= math.ceil(2652/batch_size)\n",
        "validation_steps= math.ceil(663/batch_size)\n",
        "\n",
        "history= xcp_model.fit(train_ds, validation_data= val_ds, steps_per_epoch= steps_per_epoch, epochs= epochs, callbacks= callbacks_list, validation_steps= validation_steps)"
      ],
      "metadata": {
        "id": "hOy20Hqujs8t",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc= history.history['sparse_categorical_accuracy']\n",
        "val_acc= history.history['val_sparse_categorical_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "84pHhLQCkGAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Best Epoch: 88 (model-00088-0.07750-0.99321-0.49403-0.89140.h5)\n",
        "- **Training: loss: .775 - categorical_accuracy: 0.993**\n",
        "- **Validation: val_loss: .494 - val_sparse_categorical_accuracy: 0.891**\n"
      ],
      "metadata": {
        "id": "SEb6i17RrU_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Script"
      ],
      "metadata": {
        "id": "3PfCsmCOvT3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining both the models for inference\n",
        "final_model1= tf.keras.models.load_model('/content/drive/MyDrive/colab_data/models/model-00088-0.07750-0.99321-0.49403-0.89140.h5')\n",
        "final_model2=  tf.keras.models.load_model('/content/drive/MyDrive/colab_data/models/model-00109-0.08159-0.99170-0.67942-0.85068.h5')\n",
        "models= [final_model1, final_model2]"
      ],
      "metadata": {
        "id": "ZDME1m_svRld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single inference function\n",
        "def single_inference(path, models):\n",
        "  classes = ['auditorium', 'bathroom', 'bedroom', 'elevator', 'gym', 'kitchen', 'library', 'office', 'restaurant', 'staircase']\n",
        "  img= cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "  cv2_imshow(img)\n",
        "  img= cv2.resize(img, (150, 150), interpolation = cv2.INTER_AREA)\n",
        "  img= img.reshape((1,150,150,3))\n",
        "  pred1= models[0].predict(img)\n",
        "  pred2= models[1].predict(img)\n",
        "  pred= (pred1+pred2)/2\n",
        "  print ('Predicted class:', classes[np.argmax(pred)])"
      ],
      "metadata": {
        "id": "eqY9KuwJwHb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting a random image from internet\n",
        "single_inference('/content/test1.jpg', models)"
      ],
      "metadata": {
        "id": "pzhaIbNnxSIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting a random image from internet\n",
        "single_inference('/content/test2.jpg', models)"
      ],
      "metadata": {
        "id": "7XWyEntuxzwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting a random image from internet\n",
        "single_inference('/content/test3.jpg', models)"
      ],
      "metadata": {
        "id": "94HETpWh1Xlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting a random image from internet\n",
        "single_inference('/content/test4.jpg', models)"
      ],
      "metadata": {
        "id": "_b6gsWa-8dGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E8Vz8e2wAHAh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}